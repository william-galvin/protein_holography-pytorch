
script: train_vae_aminoacids.py

slurm:
  account: stf
  partition: gpu-2080ti # ckpt, gpu-2080ti
  nodes: 1
  cores: 1
  gres: gpu:1
  walltime: '23:00:00'
  mem: 12G
  chdir: /mmfs1/gscratch/spe/gvisan01/protein_holography_e3nn

hyperparameters:
  ########## Model hyperparameters (start) ##########

  model: cgvae_symmetric

  neigh_kind: residue_only
  rmax: 20
  lmax: 4
  n_channels: 4
  rcut: 10.0
  n_train_neigh: 48317
  n_valid_neigh: 13443
  rst_normalization: square
  mul_rst_normalization: sqrt_power

  ## Most definitely not tune
  net_lmax: [4]
  linearity_first: [false]
  signal_norm: ['sqrt_power'] # [None, 'sqrt_power', 'power', 'square', 'magnitudes']
  sf_rec_loss_fn: ['mse'] # ['mse', 'cosine'] --> cosine bad
  dropout_rate: [0.0]
  nonlinearity_rule: ['full'] # ['full', 'elementwise', 'square'] --> Mike observed that 'full' has best per-parameter efficiency for classification
  weight_decay: [false]

  ## Maybe tune
  use_skip_connections_in_decoder: [false] # [false] --> HAS to be false for additive skip connections
  encoder_n_cg_blocks: [5] # [5, 7, 9] --> tune together with encoder_hidden_dim
  decoder_n_cg_blocks: [5] # [5, 7, 9] --> tune together with encoder_hidden_dim
  gate_nonlinearity_block: [null]

  batch_size: [100]
  n_epochs: [35]
  warmup_kl_epochs: [15]

  ## Definitely tune
  latent_dim: [10] # [8, 16, 32]
  encoder_hidden_dim: [25] # --> tune together with encoder_n_cg_blocks
  decoder_hidden_dim: [25] # --> tune together with encoder_n_cg_blocks
  bottleneck_hidden_dims: ['128,64,32']
  n_reconstruction_layers: [0]
  softmax_before_sf_mse: [false]
  learn_frame: [true]


  ########## Model hyperparameters (end) ##########


  ########## Learning hyperparameters (start) ##########

  lr: [0.0001]
  lr_schedule: ['decrease_after_warmup'] # constant, decrease_after_warmup

  separate_optimizing: [true] # whether to minimize kld throughout the network or only in the bottleneck
  x_rec_loss_fn: ['cosine'] # ['cosine', 'mse_normalized', 'mse'] --> mse bad
  use_batch_norm: [true]

  # [scalar_features, signal, kld]
  lambdas: ['1.0,1.0,0.3'] # ['1.0,1.0,0.1', '1.0,1.0,0.01', '1000.0,1.0,0.01'] # ['1000.0,1000.0,0.01', '1000.0,10000.0,0.01', '100.0,100.0,0.01'] # ['0.0,1.0,0.0', '0.0,1.0,0.01'] # ['1.0,1.0,0.0', '1.0,1.0,0.1', '1.0,1.0,0.01', '1000.0,1.0,0.01']
  lambdas_schedule: ['linear_up_anneal_kl'] # constant, drop_kl_at_half, linear_up_anneal_kl


  ########## Learning hyperparameters (end) ##########

